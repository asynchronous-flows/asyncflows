default_model:
  model: ollama/llama3
flow:
  # Iterate over the PDF filepaths
  extract_pdf_texts:
    for: filepath
    in:
      var: pdf_filepaths
    flow:
      # For each filepath, `extract_pdf_text` extracts text from PDF files
      extractor:
        action: extract_pdf_text
        file:
          var: filepath
  # Analyze the user's query and generate a question for the RAG system
  generate_query:
    action: prompt
    prompt:
      - heading: User's Message
        var: message
      - text: |
          Carefully analyze the user's query below and generate a clear, focused question that captures the key information needed to answer the query. 
          The question should be suitable for a retrieval system to find the most relevant documents.
  # `retrieve` performs a vector search, fast for large datasets
  retrieval:
    action: retrieve
    k: 5
    documents:
      lambda: |
        [flow.extractor.full_text for flow in extract_pdf_texts]
    query:
      var: message
  # `rerank` picks the most appropriate documents, it's slower than retrieve, but better at matching against the query
  reranking:
    action: rerank
    k: 2
    documents:
      link: retrieval.result
    query:
      var: message
  # `chatbot` prompts the LLM to summarize the top papers
  chatbot:
    action: prompt
    prompt:
      - heading: Top papers
        link: reranking.result
      - heading: Conversation history
        var: conversation_history
      - heading: New message
        var: message
      - text: |
          Based on the top papers, what is the most relevant information to the query?
          Summarize the key points of the papers in a few sentences.

default_output: chatbot.result